\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{4}
\citation{13}
\citation{15}
\citation{16}
\citation{17}
\citation{18}
\citation{19}
\citation{6}
\citation{3}
\citation{20}
\citation{21}
\citation{22}
\citation{23}
\@writefile{toc}{\thispagestyle {plain}}
\@writefile{toc}{\vspace {-.45cm}}
\citation{15}
\citation{13}
\citation{24}
\citation{13}
\citation{3}
\citation{25}
\citation{13}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Eye Physiology}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Eye Tracking and Gaze Estimation}{1}{subsection.1.2}}
\citation{1}
\citation{2}
\citation{3}
\citation{4}
\citation{7}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Diagram of light and four Purkinje images.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{pimage}{{1}{2}{Diagram of light and four Purkinje images.\relax }{figure.caption.1}{}}
\citation{13}
\citation{26}
\citation{27}
\citation{4}
\citation{5}
\citation{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Calibration Strategies}{3}{subsection.1.3}}
\citation{4}
\citation{13}
\citation{29}
\@writefile{toc}{\contentsline {section}{\numberline {2}Papers Review}{4}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Ken Pfeuffer, et.al \cite  {4}}{4}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pursuit calibration is based on moving targets (a), tolerates interruptions (b), can blend with application tasks (c) and calibrate users even when they are not aware (d).\relax }}{4}{figure.caption.2}}
\newlabel{kenpfeffer}{{2}{4}{Pursuit calibration is based on moving targets (a), tolerates interruptions (b), can blend with application tasks (c) and calibrate users even when they are not aware (d).\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Christian Lander, et.al \cite  {13}}{4}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Basic idea of hEYEbrid to use the pupil center in the corneal image (i.e. the environment reflected on the human eye) as the gaze point in the actual scene.\relax }}{5}{figure.caption.3}}
\newlabel{lander1}{{3}{5}{Basic idea of hEYEbrid to use the pupil center in the corneal image (i.e. the environment reflected on the human eye) as the gaze point in the actual scene.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Basic processing pipeline of hEYEbrid, which combines infrared eye and corneal images. The pupil is tracked in the IR image and mapped onto the corneal image to finally crop the reflection within the pupil. The mapped pupil center coincides with the gaze point.\relax }}{5}{figure.caption.4}}
\newlabel{lander2}{{4}{5}{Basic processing pipeline of hEYEbrid, which combines infrared eye and corneal images. The pupil is tracked in the IR image and mapped onto the corneal image to finally crop the reflection within the pupil. The mapped pupil center coincides with the gaze point.\relax }{figure.caption.4}{}}
\citation{15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Carlos H. Morimoto, et.al \cite  {15}}{6}{subsection.2.3}}
\newlabel{carlos1}{{5a}{6}{Dark\relax }{figure.caption.5}{}}
\newlabel{sub@carlos1}{{a}{6}{Dark\relax }{figure.caption.5}{}}
\newlabel{carlos2}{{5b}{6}{Bright\relax }{figure.caption.5}{}}
\newlabel{sub@carlos2}{{b}{6}{Bright\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dark and bright pupil images.\relax }}{6}{figure.caption.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Purkinje images.\relax }}{6}{figure.caption.6}}
\newlabel{carlos3}{{6}{6}{Purkinje images.\relax }{figure.caption.6}{}}
\citation{16}
\citation{17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Dan Witzner Hansen, et.al \cite  {16}}{7}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The shape of the eye may change drastically when viewed from different angles. For example, the eyelids may appear straight from one view but highly curved from another. The iris contour also changes with viewing angle. The dashed lines indicate when the eyelids appear straight, while the solid yellow lines represent the major axis of the iris ellipse.\relax }}{7}{figure.caption.7}}
\newlabel{danwitzner}{{7}{7}{The shape of the eye may change drastically when viewed from different angles. For example, the eyelids may appear straight from one view but highly curved from another. The iris contour also changes with viewing angle. The dashed lines indicate when the eyelids appear straight, while the solid yellow lines represent the major axis of the iris ellipse.\relax }{figure.caption.7}{}}
\citation{18}
\citation{19}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces General model of the structures of the human eye, light, light sources, and projections.\relax }}{8}{figure.caption.8}}
\newlabel{danwitzner2}{{8}{8}{General model of the structures of the human eye, light, light sources, and projections.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Feridun Mert Celebi, et.al \cite  {17}}{8}{subsection.2.5}}
\citation{19}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Augusto Esteves, et.al \cite  {18} \cite  {19}}{9}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Top: a user raises the volume of his smart watch music player using Orbits gaze input controls. The UI shows the volume, pause/play and previous/next controls with orbiting targets for gaze selection. Bottom: how Orbits enables gaze input on smart watches. The technique can robustly detect which of the controls is actively being followed by correlating each Orbits\IeC {\textquoteright } target with the user\IeC {\textquoteright }s gaze.\relax }}{9}{figure.caption.9}}
\newlabel{augusto}{{9}{9}{Top: a user raises the volume of his smart watch music player using Orbits gaze input controls. The UI shows the volume, pause/play and previous/next controls with orbiting targets for gaze selection. Bottom: how Orbits enables gaze input on smart watches. The technique can robustly detect which of the controls is actively being followed by correlating each Orbits’ target with the user’s gaze.\relax }{figure.caption.9}{}}
\citation{6}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Melodie Vidal, et.al \cite  {6}}{10}{subsection.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces When a user follows a moving object on the screen, her eyes perform the same trajectory as the followed object\IeC {\textquoteright }s.\relax }}{10}{figure.caption.10}}
\newlabel{melodie}{{10}{10}{When a user follows a moving object on the screen, her eyes perform the same trajectory as the followed object’s.\relax }{figure.caption.10}{}}
\citation{3}
\citation{9}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A user is interested in listening to a music album. She walks up to the in-store display, locates the album on the screen and follows its movement with her eyes. A sample of the music starts to play automatically.\relax }}{11}{figure.caption.11}}
\newlabel{melodie2}{{11}{11}{A user is interested in listening to a music album. She walks up to the in-store display, locates the album on the screen and follows its movement with her eyes. A sample of the music starts to play automatically.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Amer Al-Rahayfeh, et.al \cite  {3}}{11}{subsection.2.8}}
\citation{10}
\citation{10}
\citation{11}
\citation{12}
\citation{14}
\citation{20}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Christian Nitschke, et.al \cite  {20}}{12}{subsection.2.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Eye Gaze Tracking (EGT).\relax }}{13}{figure.caption.12}}
\newlabel{christian}{{12}{13}{Eye Gaze Tracking (EGT).\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Corneal Reflection Model.\relax }}{13}{figure.caption.13}}
\newlabel{christian}{{13}{13}{Corneal Reflection Model.\relax }{figure.caption.13}{}}
\citation{21}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}Jixu Chen, et.al \cite  {21}}{14}{subsection.2.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Probabilistic gaze estimation. (A) is the shown image. (B) is the saliency map $p(g|I)$ of the image. (C) is the gaze likelihood map given the optical axis. (D) is the gaze posterior probability map. The triangle shows the maximum posterior point. The circle shows the estimated gaze using the conventional method.\relax }}{14}{figure.caption.14}}
\newlabel{jixu}{{14}{14}{Probabilistic gaze estimation. (A) is the shown image. (B) is the saliency map $p(g|I)$ of the image. (C) is the gaze likelihood map given the optical axis. (D) is the gaze posterior probability map. The triangle shows the maximum posterior point. The circle shows the estimated gaze using the conventional method.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces An example of probabilistic gaze estimation result on three images. Red dots are the results of our proposed method. Blue dots are the results of the conventional method with 9-point calibration. The left column shows the gaze fixations superimposed on the original image, while the right column shows the fixations superimposed on the saliency map.\relax }}{15}{figure.caption.15}}
\newlabel{jix2}{{15}{15}{An example of probabilistic gaze estimation result on three images. Red dots are the results of our proposed method. Blue dots are the results of the conventional method with 9-point calibration. The left column shows the gaze fixations superimposed on the original image, while the right column shows the fixations superimposed on the saliency map.\relax }{figure.caption.15}{}}
\citation{22}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11}Heiko Drewes, et.al \cite  {22}}{16}{subsection.2.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Video-based eye-tracking uses the reflection of an infrared LED and the center of the pupil to calculate the direction of the eye gaze. The reflection spot stays in the same position, while the pupil moves.\relax }}{16}{figure.caption.16}}
\newlabel{heiko}{{16}{16}{Video-based eye-tracking uses the reflection of an infrared LED and the center of the pupil to calculate the direction of the eye gaze. The reflection spot stays in the same position, while the pupil moves.\relax }{figure.caption.16}{}}
\citation{23}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12}Elias D. Guestrin, et.al \cite  {23}}{17}{subsection.2.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Ray-tracing diagram (not to scale in order to be able to show all the elements of interest), showing schematic representations of an eye, a camera and a light source. Inset: eye image indicating the pupil and two corneal reflections.\relax }}{17}{figure.caption.17}}
\newlabel{elias}{{17}{17}{Ray-tracing diagram (not to scale in order to be able to show all the elements of interest), showing schematic representations of an eye, a camera and a light source. Inset: eye image indicating the pupil and two corneal reflections.\relax }{figure.caption.17}{}}
\citation{28}
\@writefile{toc}{\contentsline {section}{\numberline {3}Observations}{18}{section.3}}
\newlabel{obs1}{{18a}{18}{Reading\relax }{figure.caption.18}{}}
\newlabel{sub@obs1}{{a}{18}{Reading\relax }{figure.caption.18}{}}
\newlabel{obs2}{{18b}{18}{Watching media\relax }{figure.caption.18}{}}
\newlabel{sub@obs2}{{b}{18}{Watching media\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Sample saccade direction distributions for "reading" and "watching media" for participant 6.\relax }}{18}{figure.caption.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Sample scene images for each activity class annotated in the dataset showing the considerable variability in terms of place and time of recording. The red dot indicates the gaze location in that particular image.\relax }}{19}{figure.caption.19}}
\newlabel{obs3}{{19}{19}{Sample scene images for each activity class annotated in the dataset showing the considerable variability in terms of place and time of recording. The red dot indicates the gaze location in that particular image.\relax }{figure.caption.19}{}}
\bibcite{1}{1}
\bibcite{2}{2}
\bibcite{3}{3}
\bibcite{4}{4}
\bibcite{5}{5}
\bibcite{6}{6}
\bibcite{7}{7}
\bibcite{8}{8}
\bibcite{9}{9}
\bibcite{10}{10}
\bibcite{11}{11}
\bibcite{12}{12}
\bibcite{13}{13}
\bibcite{14}{14}
\bibcite{15}{15}
\bibcite{16}{16}
\bibcite{17}{17}
\bibcite{18}{18}
\bibcite{19}{19}
\bibcite{20}{20}
\bibcite{21}{21}
\bibcite{22}{22}
\bibcite{23}{23}
\bibcite{24}{24}
\bibcite{25}{25}
\bibcite{26}{26}
\bibcite{27}{27}
\bibcite{28}{28}
\bibcite{29}{29}
